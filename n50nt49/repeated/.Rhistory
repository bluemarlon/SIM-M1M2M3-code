coef(regfit.full1<-regsubsets(Salary~.,Hitters, method = "exhaustive", nvmax = 19),
id = which.min(summary(regfit.full1)$bic))
train.ridge.coef <- predict(glmnet(model.matrix(Salary~.,train)[,-1],train$Salary,alpha=0,lambda=grid),type="coefficients",s=bestlam.rig)[1:20,]
whole.ridge.coef <- predict(glmnet(model.matrix(Salary~.,Hitters)[,-1],Hitters$Salary,alpha=0,lambda=grid),type="coefficients",s=bestlam.rig)[1:20,]
sum((train.ridge.coef-whole.ridge.coef)^2)
##############2.c the shrinkage method parameter impacts#########
x=model.matrix(Salary~.,train)[,-1]
y=cbind(train$Salary)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
lasso.mod=glmnet(x,y,alpha=1,lambda=grid)
###we expect the large lambda will lead the coefficients to be small
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
###prediction increases##s is the penalty parameter
ridge.pred=predict(ridge.mod,s=30,newx=model.matrix(Salary~., test)[,-1])
##mape
mean(abs(ridge.pred-test$Salary))
######with higher penalty, the coefficients tend to be small#
sum((predict(ridge.mod,type="coefficients",s=100)[1:20,])[-1]^2)
sum((predict(ridge.mod,type="coefficients",s=1000)[1:20,])[-1]^2)
##############2.d CV select parameter, and the coefs plots#############
####CV
set.seed(1)
cv.out=cv.glmnet(x,y,alpha=0,type.measure="mse")
# plot(cv.out)
bestlam.rig=cv.out$lambda.min
ridge.pred=predict(ridge.mod,s=bestlam.rig ,newx=model.matrix(Salary~., test)[,-1])
mean(abs(ridge.pred-test$Salary))
predict(ridge.mod,type="coefficients",s=bestlam.rig)[1:20,]
summary(predict(ridge.mod,type="coefficients",s=bestlam.rig)[1:20,])
set.seed(1)
cv.out=cv.glmnet(x,y,alpha=1,type.measure="mse")
bestlam.las=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam.las ,newx=model.matrix(Salary~., test)[,-1])
mean(abs(lasso.pred-test$Salary))
predict(lasso.mod,type="coefficients",s=bestlam.las)[1:20,]
ridge.coef = as.matrix(coef.glmnet(ridge.mod))
lasso.coef = as.matrix(coef.glmnet(lasso.mod))
log.lambda = log(ridge.mod$lambda[order(ridge.mod$lambda, decreasing = TRUE)])
par(mfrow = c(1,2))
plot(log.lambda,
ridge.coef[2,], type = "l", ylim = c(min(ridge.coef[-1,]), max(ridge.coef[-1,])), ylab = "ridge")
for(k in 3:20) lines(log.lambda,ridge.coef[k,], col = k)
abline(v = log(bestlam.rig))
plot(log.lambda,
lasso.coef[2,], type = "l", ylim = c(min(lasso.coef[-1,]), max(lasso.coef[-1,])), ylab = "lasso")
for(k in 3:20) lines(log.lambda,lasso.coef[k,], col = k)
abline(v=log(bestlam.las))
# #######coefs of the shrinkage methods###
# ridge.coef[,max(which(ridge.mod$lambda[order(ridge.mod$lambda, decreasing = TRUE)]>=bestlam.rig))]
# lasso.coef[,max(which(ridge.mod$lambda[order(ridge.mod$lambda, decreasing = TRUE)]>=bestlam.las))]
coef(regfit.full1<-regsubsets(Salary~.,train, method = "exhaustive", nvmax = 19),
id = which.min(summary(regfit.full1)$bic))
coef(regfit.full1<-regsubsets(Salary~.,Hitters, method = "exhaustive", nvmax = 19),
id = which.min(summary(regfit.full1)$bic))
train.ridge.coef <- predict(glmnet(model.matrix(Salary~.,train)[,-1],train$Salary,alpha=0,lambda=grid),type="coefficients",s=bestlam.rig)[1:20,]
whole.ridge.coef <- predict(glmnet(model.matrix(Salary~.,Hitters)[,-1],Hitters$Salary,alpha=0,lambda=grid),type="coefficients",s=bestlam.rig)[1:20,]
(train.ridge.coef <- predict(glmnet(model.matrix(Salary~.,train)[,-1],train$Salary,alpha=0,lambda=grid),type="coefficients",s=bestlam.rig)[1:20,])
(whole.ridge.coef <- predict(glmnet(model.matrix(Salary~.,Hitters)[,-1],Hitters$Salary,alpha=0,lambda=grid),type="coefficients",s=bestlam.rig)[1:20,])
sum((train.ridge.coef-whole.ridge.coef)^2)
#########################################################################################################
###################################1. stagewise #########################################################
###################################2. shrinkage #########################################################
###################################2.a time cost comparison #############################################
###################################2.b dataset change the shrinkage tends to be more stable #############
###################################2.c the shrinkage method parameter impacts############################
###################################2.d CV select parameter, and the coefs plots##########################
#########################################################################################################
library(ISLR)
# fix(Hitters)
#############load the dataset###
names(Hitters)
dim(Hitters)
###remove the missing observations###
Hitters=na.omit(Hitters)
Credit = na.omit(Credit)
####normalize the numeric predictors###
Hitters <- cbind(scale(Hitters[,-c(14,15,19,20)]), Hitters[,c(14,15,19,20)])
# major league baseball data from 1986 to 1987 seasons;
# various statistics associated with performance in the previous year
dim(Hitters)
library(leaps)
######split the dataset into training and hold out ###
set.seed(1231)
train.id = sample(1:nrow(Hitters),size = (nrow(Hitters)*9) %/% 10, replace = FALSE)
train = Hitters[train.id,]
test = Hitters[-train.id,]
###########1. stagewise#############
#######use the training to train the model
regfit.full=regsubsets(Salary~.,train, method = "exhaustive", nvmax = 19)
# summary(regfit.full)
regfit.fwd=regsubsets(Salary~.,train, method = "forward", nvmax = 19)
regfit.bwd=regsubsets(Salary~.,train, method = "backward", nvmax = 19)
regfit.seq=regsubsets(Salary~.,train, method = "seqrep", nvmax = 19)
min(summary(regfit.full)$bic)
min(summary(regfit.fwd)$bic)
min(summary(regfit.bwd)$bic)
min(summary(regfit.seq)$bic)
###########make some simple code to predict###
predict.regsubsets =function (object ,newdata ,id ,...){
form=as.formula(object$call [[2]])
mat=model.matrix(form,newdata)
coefi=coef(object ,id=id)
xvars=names(coefi)
mat[,xvars]%*%coefi
}
# ####the best subset model on training may not be the best on testing over stepwise model
# mean(abs(test$Salary - predict(regfit.full, test, id = which.min(summary(regfit.full)$bic))))
# mean(abs(test$Salary - predict(regfit.fwd, test, id = which.min(summary(regfit.fwd)$bic))))
# mean(abs(test$Salary - predict(regfit.bwd, test, id = which.min(summary(regfit.bwd)$bic))))
# mean(abs(test$Salary - predict(regfit.seq, test, id = which.min(summary(regfit.seq)$bic))))
##########look at the model selected predictors and coefs if needed###
# coef(regfit.full, id = which.min(summary(regfit.full)$bic))
# coef(regfit.full1<-regsubsets(Salary~.,Hitters, method = "exhaustive", nvmax = 19),
#      id = which.min(summary(regfit.full1)$bic))
#################2. shrinkage############
library(glmnet)
set.seed(1234)
X<-matrix(rnorm(n=10^5*19), ncol = 19)
train.large = data.frame(cbind(X, tcrossprod(X, t(1:19))))
grid=10^seq(10,-2,length=100)
############2.a time cost comparison########
colnames(train.large)[20] = "Salary"
system.time(regsubsets(Salary~.,data = train.large, method = "exhaustive", nvmax = 19))
system.time(regsubsets(Salary~.,train.large, method = "seqrep", nvmax = 19))
system.time(glmnet(X,train.large$Salary,alpha=0,lambda=grid))##ridge
system.time(glmnet(X,train.large$Salary,alpha=1,lambda=grid))##lasso
##############2.b dataset change the shrinkage tends to be more stable#######
coef(regfit.full1<-regsubsets(Salary~.,train, method = "exhaustive", nvmax = 19),
id = which.min(summary(regfit.full1)$bic))
coef(regfit.full1<-regsubsets(Salary~.,Hitters, method = "exhaustive", nvmax = 19),
id = which.min(summary(regfit.full1)$bic))
(train.ridge.coef <- predict(glmnet(model.matrix(Salary~.,train)[,-1],train$Salary,alpha=0,lambda=grid),type="coefficients",s=bestlam.rig)[1:20,])
(whole.ridge.coef <- predict(glmnet(model.matrix(Salary~.,Hitters)[,-1],Hitters$Salary,alpha=0,lambda=grid),type="coefficients",s=bestlam.rig)[1:20,])
sum((train.ridge.coef-whole.ridge.coef)^2)
##############2.c the shrinkage method parameter impacts#########
x=model.matrix(Salary~.,train)[,-1]
y=cbind(train$Salary)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
lasso.mod=glmnet(x,y,alpha=1,lambda=grid)
###we expect the large lambda will lead the coefficients to be small
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
###prediction increases##s is the penalty parameter
ridge.pred=predict(ridge.mod,s=30,newx=model.matrix(Salary~., test)[,-1])
##mape
mean(abs(ridge.pred-test$Salary))
######with higher penalty, the coefficients tend to be small#
sum((predict(ridge.mod,type="coefficients",s=100)[1:20,])[-1]^2)
sum((predict(ridge.mod,type="coefficients",s=1000)[1:20,])[-1]^2)
##############2.d CV select parameter, and the coefs plots#############
####CV
set.seed(1)
cv.out=cv.glmnet(x,y,alpha=0,type.measure="mse")
# plot(cv.out)
bestlam.rig=cv.out$lambda.min
ridge.pred=predict(ridge.mod,s=bestlam.rig ,newx=model.matrix(Salary~., test)[,-1])
mean(abs(ridge.pred-test$Salary))
predict(ridge.mod,type="coefficients",s=bestlam.rig)[1:20,]
summary(predict(ridge.mod,type="coefficients",s=bestlam.rig)[1:20,])
set.seed(1)
cv.out=cv.glmnet(x,y,alpha=1,type.measure="mse")
bestlam.las=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam.las ,newx=model.matrix(Salary~., test)[,-1])
mean(abs(lasso.pred-test$Salary))
predict(lasso.mod,type="coefficients",s=bestlam.las)[1:20,]
ridge.coef = as.matrix(coef.glmnet(ridge.mod))
lasso.coef = as.matrix(coef.glmnet(lasso.mod))
log.lambda = log(ridge.mod$lambda[order(ridge.mod$lambda, decreasing = TRUE)])
par(mfrow = c(1,2))
plot(log.lambda,
ridge.coef[2,], type = "l", ylim = c(min(ridge.coef[-1,]), max(ridge.coef[-1,])), ylab = "ridge")
for(k in 3:20) lines(log.lambda,ridge.coef[k,], col = k)
abline(v = log(bestlam.rig))
plot(log.lambda,
lasso.coef[2,], type = "l", ylim = c(min(lasso.coef[-1,]), max(lasso.coef[-1,])), ylab = "lasso")
for(k in 3:20) lines(log.lambda,lasso.coef[k,], col = k)
abline(v=log(bestlam.las))
# #######coefs of the shrinkage methods###
# ridge.coef[,max(which(ridge.mod$lambda[order(ridge.mod$lambda, decreasing = TRUE)]>=bestlam.rig))]
# lasso.coef[,max(which(ridge.mod$lambda[order(ridge.mod$lambda, decreasing = TRUE)]>=bestlam.las))]
#########################################################################################################
###################################1. stagewise #########################################################
###################################2. shrinkage #########################################################
###################################2.a time cost comparison #############################################
###################################2.b dataset change the shrinkage tends to be more stable #############
###################################2.c the shrinkage method parameter impacts############################
###################################2.d CV select parameter, and the coefs plots##########################
#########################################################################################################
library(ISLR)
# fix(Hitters)
#############load the dataset###
names(Hitters)
dim(Hitters)
###remove the missing observations###
Hitters=na.omit(Hitters)
Credit = na.omit(Credit)
####normalize the numeric predictors###
Hitters <- cbind(scale(Hitters[,-c(14,15,19,20)]), Hitters[,c(14,15,19,20)])
# major league baseball data from 1986 to 1987 seasons;
# various statistics associated with performance in the previous year
dim(Hitters)
library(leaps)
######split the dataset into training and hold out ###
set.seed(1231)
train.id = sample(1:nrow(Hitters),size = (nrow(Hitters)*9) %/% 10, replace = FALSE)
train = Hitters[train.id,]
test = Hitters[-train.id,]
###########1. stagewise#############
#######use the training to train the model
regfit.full=regsubsets(Salary~.,train, method = "exhaustive", nvmax = 19)
# summary(regfit.full)
regfit.fwd=regsubsets(Salary~.,train, method = "forward", nvmax = 19)
regfit.bwd=regsubsets(Salary~.,train, method = "backward", nvmax = 19)
regfit.seq=regsubsets(Salary~.,train, method = "seqrep", nvmax = 19)
min(summary(regfit.full)$bic)
min(summary(regfit.fwd)$bic)
min(summary(regfit.bwd)$bic)
min(summary(regfit.seq)$bic)
###########make some simple code to predict###
predict.regsubsets =function (object ,newdata ,id ,...){
form=as.formula(object$call [[2]])
mat=model.matrix(form,newdata)
coefi=coef(object ,id=id)
xvars=names(coefi)
mat[,xvars]%*%coefi
}
# ####the best subset model on training may not be the best on testing over stepwise model
# mean(abs(test$Salary - predict(regfit.full, test, id = which.min(summary(regfit.full)$bic))))
# mean(abs(test$Salary - predict(regfit.fwd, test, id = which.min(summary(regfit.fwd)$bic))))
# mean(abs(test$Salary - predict(regfit.bwd, test, id = which.min(summary(regfit.bwd)$bic))))
# mean(abs(test$Salary - predict(regfit.seq, test, id = which.min(summary(regfit.seq)$bic))))
##########look at the model selected predictors and coefs if needed###
# coef(regfit.full, id = which.min(summary(regfit.full)$bic))
# coef(regfit.full1<-regsubsets(Salary~.,Hitters, method = "exhaustive", nvmax = 19),
#      id = which.min(summary(regfit.full1)$bic))
#################2. shrinkage############
library(glmnet)
set.seed(1234)
X<-matrix(rnorm(n=10^5*19), ncol = 19)
train.large = data.frame(cbind(X, tcrossprod(X, t(1:19))))
grid=10^seq(10,-2,length=100)
############2.a time cost comparison########
colnames(train.large)[20] = "Salary"
system.time(regsubsets(Salary~.,data = train.large, method = "exhaustive", nvmax = 19))
system.time(regsubsets(Salary~.,train.large, method = "seqrep", nvmax = 19))
system.time(glmnet(X,train.large$Salary,alpha=0,lambda=grid))##ridge
system.time(glmnet(X,train.large$Salary,alpha=1,lambda=grid))##lasso
##############2.b dataset change the shrinkage tends to be more stable#######
coef(regfit.full1<-regsubsets(Salary~.,train, method = "exhaustive", nvmax = 19),
id = which.min(summary(regfit.full1)$bic))
coef(regfit.full1<-regsubsets(Salary~.,Hitters, method = "exhaustive", nvmax = 19),
id = which.min(summary(regfit.full1)$bic))
(train.ridge.coef <- predict(glmnet(model.matrix(Salary~.,train)[,-1],train$Salary,alpha=0,lambda=grid),type="coefficients",s=bestlam.rig)[1:20,])
(whole.ridge.coef <- predict(glmnet(model.matrix(Salary~.,Hitters)[,-1],Hitters$Salary,alpha=0,lambda=grid),type="coefficients",s=bestlam.rig)[1:20,])
sum((train.ridge.coef-whole.ridge.coef)^2)
##############2.c the shrinkage method parameter impacts#########
x=model.matrix(Salary~.,train)[,-1]
y=cbind(train$Salary)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
lasso.mod=glmnet(x,y,alpha=1,lambda=grid)
###we expect the large lambda will lead the coefficients to be small
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
###prediction increases##s is the penalty parameter
ridge.pred=predict(ridge.mod,s=30,newx=model.matrix(Salary~., test)[,-1])
##mape
mean(abs(ridge.pred-test$Salary))
######with higher penalty, the coefficients tend to be small#
sum((predict(ridge.mod,type="coefficients",s=100)[1:20,])[-1]^2)
sum((predict(ridge.mod,type="coefficients",s=1000)[1:20,])[-1]^2)
##############2.d CV select parameter, and the coefs plots#############
####CV
set.seed(1)
cv.out=cv.glmnet(x,y,alpha=0,type.measure="mse")
# plot(cv.out)
bestlam.rig=cv.out$lambda.min
ridge.pred=predict(ridge.mod,s=bestlam.rig ,newx=model.matrix(Salary~., test)[,-1])
mean(abs(ridge.pred-test$Salary))
predict(ridge.mod,type="coefficients",s=bestlam.rig)[1:20,]
summary(predict(ridge.mod,type="coefficients",s=bestlam.rig)[1:20,])
set.seed(1)
cv.out=cv.glmnet(x,y,alpha=1,type.measure="mse")
bestlam.las=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam.las ,newx=model.matrix(Salary~., test)[,-1])
mean(abs(lasso.pred-test$Salary))
predict(lasso.mod,type="coefficients",s=bestlam.las)[1:20,]
ridge.coef = as.matrix(coef.glmnet(ridge.mod))
lasso.coef = as.matrix(coef.glmnet(lasso.mod))
log.lambda = log(ridge.mod$lambda[order(ridge.mod$lambda, decreasing = TRUE)])
par(mfrow = c(1,2))
plot(log.lambda,
ridge.coef[2,], type = "l", ylim = c(min(ridge.coef[-1,]), max(ridge.coef[-1,])), ylab = "ridge")
for(k in 3:20) lines(log.lambda,ridge.coef[k,], col = k)
abline(v = log(bestlam.rig))
plot(log.lambda,
lasso.coef[2,], type = "l", ylim = c(min(lasso.coef[-1,]), max(lasso.coef[-1,])), ylab = "lasso")
for(k in 3:20) lines(log.lambda,lasso.coef[k,], col = k)
abline(v=log(bestlam.las))
# #######coefs of the shrinkage methods###
# ridge.coef[,max(which(ridge.mod$lambda[order(ridge.mod$lambda, decreasing = TRUE)]>=bestlam.rig))]
# lasso.coef[,max(which(ridge.mod$lambda[order(ridge.mod$lambda, decreasing = TRUE)]>=bestlam.las))]
coef(regfit.full1<-regsubsets(Salary~.,train, method = "exhaustive", nvmax = 19),
id = which.min(summary(regfit.full1)$bic))
coef(regfit.full1<-regsubsets(Salary~.,Hitters, method = "exhaustive", nvmax = 19),
id = which.min(summary(regfit.full1)$bic))
(train.ridge.coef <- predict(glmnet(model.matrix(Salary~.,train)[,-1],train$Salary,alpha=0,lambda=grid),type="coefficients",s=bestlam.rig)[1:20,])
(whole.ridge.coef <- predict(glmnet(model.matrix(Salary~.,Hitters)[,-1],Hitters$Salary,alpha=0,lambda=grid),type="coefficients",s=bestlam.rig)[1:20,])
sum((train.ridge.coef-whole.ridge.coef)^2)
system.time(regsubsets(Salary~.,data = train.large, method = "exhaustive", nvmax = 19))
system.time(regsubsets(Salary~.,train.large, method = "seqrep", nvmax = 19))
system.time(glmnet(X,train.large$Salary,alpha=0,lambda=grid))##ridge
system.time(glmnet(X,train.large$Salary,alpha=1,lambda=grid))##lasso
?regsubsets
library(ISLR)
# fix(Hitters)
#############load the dataset###
names(Hitters)
dim(Hitters)
###remove the missing observations###
Hitters=na.omit(Hitters)
Credit = na.omit(Credit)
####normalize the numeric predictors###
Hitters <- cbind(scale(Hitters[,-c(14,15,19,20)]), Hitters[,c(14,15,19,20)])
# major league baseball data from 1986 to 1987 seasons;
# various statistics associated with performance in the previous year
dim(Hitters)
library(leaps)
######split the dataset into training and hold out ###
set.seed(1231)
train.id = sample(1:nrow(Hitters),size = (nrow(Hitters)*9) %/% 10, replace = FALSE)
train = Hitters[train.id,]
test = Hitters[-train.id,]
###########1. stagewise#############
#######use the training to train the model
regfit.full=regsubsets(Salary~.,train, method = "exhaustive", nvmax = 19)
# summary(regfit.full)
regfit.fwd=regsubsets(Salary~.,train, method = "forward", nvmax = 19)
regfit.bwd=regsubsets(Salary~.,train, method = "backward", nvmax = 19)
regfit.seq=regsubsets(Salary~.,train, method = "seqrep", nvmax = 19)
min(summary(regfit.full)$bic)
min(summary(regfit.fwd)$bic)
min(summary(regfit.bwd)$bic)
min(summary(regfit.seq)$bic)
###########make some simple code to predict###
predict.regsubsets =function (object ,newdata ,id ,...){
form=as.formula(object$call [[2]])
mat=model.matrix(form,newdata)
coefi=coef(object ,id=id)
xvars=names(coefi)
mat[,xvars]%*%coefi
}
library(glmnet)
set.seed(1234)
X<-matrix(rnorm(n=10^5*19), ncol = 19)
train.large = data.frame(cbind(X, tcrossprod(X, t(1:19))))
grid=10^seq(10,-2,length=100)
colnames(train.large)[20] = "Salary"
coef(regfit.full1<-regsubsets(Salary~.,train, method = "exhaustive", nvmax = 19),
id = which.min(summary(regfit.full1)$bic))
coef(regfit.full1<-regsubsets(Salary~.,Hitters, method = "exhaustive", nvmax = 19),
id = which.min(summary(regfit.full1)$bic))
(train.ridge.coef <- predict(glmnet(model.matrix(Salary~.,train)[,-1],train$Salary,alpha=0,lambda=grid),type="coefficients",s=bestlam.rig)[1:20,])
system.time(regsubsets(Salary~.,data = train.large, method = "exhaustive", nvmax = 19))
system.time(regsubsets(Salary~.,train.large, method = "seqrep", nvmax = 19))
system.time(cv.glmnet(x,y,alpha=0,type.measure="mse"))##ridge
#########################################################################################################
###################################1. stepwise #########################################################
###################################2. shrinkage #########################################################
###################################2.a the shrinkage method parameter impacts############################
###################################2.b CV select parameter, and the coefs plots##########################
###################################2.c time cost comparison #############################################
###################################2.d dataset change the shrinkage tends to be more stable #############
#########################################################################################################
library(ISLR)
# fix(Hitters)
#############load the dataset###
names(Hitters)
dim(Hitters)
###remove the missing observations###
Hitters=na.omit(Hitters)
Credit = na.omit(Credit)
####normalize the numeric predictors###
Hitters <- cbind(scale(Hitters[,-c(14,15,19,20)]), Hitters[,c(14,15,19,20)])
# major league baseball data from 1986 to 1987 seasons;
# various statistics associated with performance in the previous year
dim(Hitters)
library(leaps)
######split the dataset into training and hold out ###
set.seed(1231)
train.id = sample(1:nrow(Hitters),size = (nrow(Hitters)*9) %/% 10, replace = FALSE)
train = Hitters[train.id,]
test = Hitters[-train.id,]
###########1. stagewise#############
#######use the training to train the model
regfit.full=regsubsets(Salary~.,train, method = "exhaustive", nvmax = 19)
# summary(regfit.full)
regfit.fwd=regsubsets(Salary~.,train, method = "forward", nvmax = 19)
regfit.bwd=regsubsets(Salary~.,train, method = "backward", nvmax = 19)
regfit.seq=regsubsets(Salary~.,train, method = "seqrep", nvmax = 19)
min(summary(regfit.full)$bic)
min(summary(regfit.fwd)$bic)
min(summary(regfit.bwd)$bic)
min(summary(regfit.seq)$bic)
###########make some simple code to predict###
predict.regsubsets =function (object ,newdata ,id ,...){
form=as.formula(object$call [[2]])
mat=model.matrix(form,newdata)
coefi=coef(object ,id=id)
xvars=names(coefi)
mat[,xvars]%*%coefi
}
# ####the best subset model on training may not be the best on testing over stepwise model
# mean(abs(test$Salary - predict(regfit.full, test, id = which.min(summary(regfit.full)$bic))))
# mean(abs(test$Salary - predict(regfit.fwd, test, id = which.min(summary(regfit.fwd)$bic))))
# mean(abs(test$Salary - predict(regfit.bwd, test, id = which.min(summary(regfit.bwd)$bic))))
# mean(abs(test$Salary - predict(regfit.seq, test, id = which.min(summary(regfit.seq)$bic))))
##########look at the model selected predictors and coefs if needed###
# coef(regfit.full, id = which.min(summary(regfit.full)$bic))
# coef(regfit.full1<-regsubsets(Salary~.,Hitters, method = "exhaustive", nvmax = 19),
#      id = which.min(summary(regfit.full1)$bic))
#################2. shrinkage############
library(glmnet)
set.seed(1234)
X<-matrix(rnorm(n=10^5*19), ncol = 19)
train.large = data.frame(cbind(X, tcrossprod(X, t(1:19))))
grid=10^seq(10,-2,length=100)
colnames(train.large)[20] = "Salary"
##############2.a the shrinkage method parameter impacts#########
x=model.matrix(Salary~.,train)[,-1]
y=cbind(train$Salary)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
lasso.mod=glmnet(x,y,alpha=1,lambda=grid)
###we expect the large lambda will lead the coefficients to be small
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
###prediction increases##s is the penalty parameter
ridge.pred=predict(ridge.mod,s=30,newx=model.matrix(Salary~., test)[,-1])
##mape
mean(abs(ridge.pred-test$Salary))
######with higher penalty, the coefficients tend to be small#
sum((predict(ridge.mod,type="coefficients",s=100)[1:20,])[-1]^2)
sum((predict(ridge.mod,type="coefficients",s=1000)[1:20,])[-1]^2)
##############2.b CV select parameter, and the coefs plots#############
####CV
set.seed(1)
cv.out=cv.glmnet(x,y,alpha=0,type.measure="mse")
# plot(cv.out)
bestlam.rig=cv.out$lambda.min
ridge.pred=predict(ridge.mod,s=bestlam.rig ,newx=model.matrix(Salary~., test)[,-1])
mean(abs(ridge.pred-test$Salary))
predict(ridge.mod,type="coefficients",s=bestlam.rig)[1:20,]
summary(predict(ridge.mod,type="coefficients",s=bestlam.rig)[1:20,])
set.seed(1)
cv.out=cv.glmnet(x,y,alpha=1,type.measure="mse")
bestlam.las=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam.las ,newx=model.matrix(Salary~., test)[,-1])
mean(abs(lasso.pred-test$Salary))
predict(lasso.mod,type="coefficients",s=bestlam.las)[1:20,]
ridge.coef = as.matrix(coef.glmnet(ridge.mod))
lasso.coef = as.matrix(coef.glmnet(lasso.mod))
log.lambda = log(ridge.mod$lambda[order(ridge.mod$lambda, decreasing = TRUE)])
par(mfrow = c(1,2))
plot(log.lambda,
ridge.coef[2,], type = "l", ylim = c(min(ridge.coef[-1,]), max(ridge.coef[-1,])), ylab = "ridge")
for(k in 3:20) lines(log.lambda,ridge.coef[k,], col = k)
abline(v = log(bestlam.rig))
plot(log.lambda,
lasso.coef[2,], type = "l", ylim = c(min(lasso.coef[-1,]), max(lasso.coef[-1,])), ylab = "lasso")
for(k in 3:20) lines(log.lambda,lasso.coef[k,], col = k)
abline(v=log(bestlam.las))
# #######coefs of the shrinkage methods###
# ridge.coef[,max(which(ridge.mod$lambda[order(ridge.mod$lambda, decreasing = TRUE)]>=bestlam.rig))]
# lasso.coef[,max(which(ridge.mod$lambda[order(ridge.mod$lambda, decreasing = TRUE)]>=bestlam.las))]
############2.c time cost comparison########
system.time(regsubsets(Salary~.,data = train.large, method = "exhaustive", nvmax = 19))
system.time(regsubsets(Salary~.,train.large, method = "seqrep", nvmax = 19))
system.time(cv.glmnet(x,y,alpha=0,type.measure="mse"))##ridge
system.time(cv.glmnet(x,y,alpha=1,type.measure="mse"))##lasso
##############2.d dataset change the shrinkage tends to be more stable#######
coef(regfit.full1<-regsubsets(Salary~.,train, method = "exhaustive", nvmax = 19),
id = which.min(summary(regfit.full1)$bic))
coef(regfit.full1<-regsubsets(Salary~.,Hitters, method = "exhaustive", nvmax = 19),
id = which.min(summary(regfit.full1)$bic))
(train.ridge.coef <- predict(glmnet(model.matrix(Salary~.,train)[,-1],train$Salary,alpha=0,lambda=grid),type="coefficients",s=bestlam.rig)[1:20,])
(whole.ridge.coef <- predict(glmnet(model.matrix(Salary~.,Hitters)[,-1],Hitters$Salary,alpha=0,lambda=grid),type="coefficients",s=bestlam.rig)[1:20,])
sum((train.ridge.coef-whole.ridge.coef)^2)
setwd("~/Dropbox/Renjie Chen/Ofer Harel 2016/simulate/whole/gaussiansimulatedata/static")
data.day <- read.csv("staticont2iid.csv")
summary(lmer(yjt ~ ZQ7 +ZTox3item+(0+ZQ7|PID), data = data.day))
library(lme4)
summary(lmer(yjt ~ ZQ7 +ZTox3item+(0+ZQ7|PID), data = data.day))
data.day <- read.csv("staticont2iid.csv")
library(nlme)
summary(lme(yjt ~ ZQ7 +ZTox3item, random = ~0+ZQ7|PID, data = data.day))
data.day <- read.csv("staticont2iid.csv")
library(blme)
summary(blmer(yjt ~ ZQ7 +ZTox3item+(0+ZQ7|PID), data = data.day))
library(MCMCglmm)
data.day <- read.csv("staticont2iid.csv")
library(MCMCglmm)
summary(MCMCglmm(yjt ~ ZQ7 +ZTox3item, random = ~ZQ7:PID, data = data.day, verbose = FALSE))
library(MASS)
summary(glmmPQL(yjt ~ ZQ7 +ZTox3item,random = ~0+ZQ7|PID, data = data.day, family = gaussian))
setwd("~/Dropbox/Renjie Chen/Ofer Harel 2016/simulate/whole/gaussiansimulatedata/repeated")
data.day <- read.csv("ofercontsim.csv")
library(nlme)
summary(lme(MissedDose ~ ZQ7 +ZTox3item, random = ~0+ ZQ7|PID,
correlation = corAR1(form = ~Day|PID), data = data.day))
library(MASS)
summary(glmmPQL(MissedDose ~ ZQ7 +ZTox3item,random = ~0+ZQ7|PID,
correlation = corAR1(form = ~Day|PID),data = data.day, family = gaussian))
